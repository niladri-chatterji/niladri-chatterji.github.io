[{"authors":["admin"],"categories":null,"content":"(I am currently on the job market!)\nI am a postdoctoral researcher at Stanford University. I am associated with the Stanford Artificial Intelligence Lab and work with Tatsu Hashimoto and Percy Liang.\nMy research interests lie at the intersection of Machine Learning and Statistics. My current research interests center around building a theory to understand and improve neural network models. Many neural network models generalize well despite perfectly fitting noisy training data; I am interested in understanding what leads to successful generalization and optimization. I also develop techniques to make neural networks robust and think about scaling laws for neural networks.\nSome areas I have worked on in the past include Markov chain Monte Carlo methods, optimization for structured non-convex problems, and online learning.\nPreviously I spent six wonderful years at UC Berkeley advised by Peter Bartlett. Before that, I graduated from the Indian Institute of Technology Bombay in 2015 with a dual degree B.Tech+M.Tech in Engineering Physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"(I am currently on the job market!)\nI am a postdoctoral researcher at Stanford University. I am associated with the Stanford Artificial Intelligence Lab and work with Tatsu Hashimoto and Percy Liang.\nMy research interests lie at the intersection of Machine Learning and Statistics. My current research interests center around building a theory to understand and improve neural network models. Many neural network models generalize well despite perfectly fitting noisy training data; I am interested in understanding what leads to successful generalization and optimization.","tags":null,"title":"Niladri S. Chatterji","type":"authors"},{"authors":["Liang et al."],"categories":null,"content":"","date":1668556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668556800,"objectID":"c1d2f655371103ca39f49f1aff3beeb5","permalink":"/publication/helm/","publishdate":"2022-11-16T00:00:00Z","relpermalink":"/publication/helm/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"Holistic evaluation of language models","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"b390c670f7949f715dce2d0a8443937a","permalink":"/publication/deep_shallow_benign/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/publication/deep_shallow_benign/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"Deep linear networks can benignly overfit when shallow ones do","type":"publication"},{"authors":["Niladri S. Chatterji*","Saminul Haque*","Tatsunori Hashimoto"],"categories":null,"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"d41da387962663b1759283c8d33e9c99","permalink":"/publication/undersampling_minimax/","publishdate":"2022-05-26T00:00:00Z","relpermalink":"/publication/undersampling_minimax/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"Undersampling is a minimax optimal robustness intervention in nonparametric classification","type":"publication"},{"authors":["Spencer Frei","Niladri S. Chatterji","Peter L. Bartlett"],"categories":null,"content":"","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644883200,"objectID":"b3ee1495be3a90695b3d32dc051d7199","permalink":"/publication/random_feature_amp/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/publication/random_feature_amp/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"Random feature amplification: Feature learning and generalization in neural networks","type":"publication"},{"authors":["Spencer Frei","Niladri S. Chatterji","Peter L. Bartlett"],"categories":null,"content":"","date":1644537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644537600,"objectID":"88c5c7122f0f99e11ca506f98acc47e0","permalink":"/publication/leaky_benign/","publishdate":"2022-02-11T00:00:00Z","relpermalink":"/publication/leaky_benign/","section":"publication","summary":"COLT 2022","tags":null,"title":"Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data","type":"publication"},{"authors":["Ke Alexander Wang*","Niladri S. Chatterji*","Saminul Haque","Tatsunori Hashimoto"],"categories":null,"content":"","date":1640304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640304000,"objectID":"4881c8ccd1e3da5d2e4fc209f13b1135","permalink":"/publication/importance_weights_polynomial/","publishdate":"2021-12-24T00:00:00Z","relpermalink":"/publication/importance_weights_polynomial/","section":"publication","summary":"ICLR 2022; also presented as a spotlight talk in the Workshop on Distribution Shifts, NeurIPS 2021","tags":null,"title":"Is importance weighting incompatible with interpolating classifiers?","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long"],"categories":null,"content":"","date":1633478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633478400,"objectID":"4089ae0143f6af8be36e872335d81531","permalink":"/publication/foolish_crowd/","publishdate":"2021-10-06T00:00:00Z","relpermalink":"/publication/foolish_crowd/","section":"publication","summary":"Journal of Machine Learning Research; also presented at NeurIPS 2022","tags":null,"title":"Foolish crowds support benign overfitting","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long","Peter L. Bartlett"],"categories":null,"content":"","date":1629936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629936000,"objectID":"8c110a9609bf1c0ffd4fe48637b9ea8f","permalink":"/publication/benign_two_layer/","publishdate":"2021-08-26T00:00:00Z","relpermalink":"/publication/benign_two_layer/","section":"publication","summary":"Journal of Machine Learning Research","tags":null,"title":"The interplay between implicit bias and benign overfitting in two-layer linear networks","type":"publication"},{"authors":["Bommasani et al."],"categories":null,"content":"","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072000,"objectID":"fe5be53e63c866efb7afc635e56757c8","permalink":"/publication/foundation_models/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/publication/foundation_models/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"On the opportunities and risks of foundation models","type":"publication"},{"authors":["Niladri S. Chatterji*","Aldo Pacchiano*","Peter L. Bartlett","Michael I. Jordan"],"categories":null,"content":"","date":1622419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622419200,"objectID":"6540b648b3aa26755bcbb73eb463b169","permalink":"/publication/rl_trajectories/","publishdate":"2021-05-31T00:00:00Z","relpermalink":"/publication/rl_trajectories/","section":"publication","summary":"NeurIPS 2021; also presented as an oral talk in the Workshop on Reinforcement Learning Theory, ICML 2021","tags":null,"title":"On the theory of reinforcement learning with once-per-episode feedback","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long","Peter L. Bartlett"],"categories":null,"content":"","date":1612828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612828800,"objectID":"05f32766194b877fddbb0ebc1be010dd","permalink":"/publication/deep_interpolation/","publishdate":"2021-02-09T00:00:00Z","relpermalink":"/publication/deep_interpolation/","section":"publication","summary":"COLT 2021","tags":null,"title":"When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long","Peter L. Bartlett"],"categories":null,"content":"","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"d78e2e5502a73ccb1129d81f01e89504","permalink":"/publication/rich_interpolation/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/publication/rich_interpolation/","section":"publication","summary":"Journal of Machine Learning Research","tags":null,"title":"When does gradient descent with logistic loss find interpolating two-layer networks?","type":"publication"},{"authors":["Niladri S. Chatterji","Philip M. Long"],"categories":null,"content":"","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"c886041c77a1b2cbefb5442295c94a5f","permalink":"/publication/classification_overparameterized/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/classification_overparameterized/","section":"publication","summary":"Journal of Machine Learning Research","tags":null,"title":"Finite-sample analysis of interpolating linear classifiers in the overparameterized regime","type":"publication"},{"authors":["Niladri S. Chatterji","Peter L. Bartlett","Philip M. Long"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"9069b2d33a2c2391e5f64972d3118d8b","permalink":"/publication/lower_bound_mcmc/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/lower_bound_mcmc/","section":"publication","summary":"Bernoulli","tags":null,"title":"Oracle lower bounds for stochastic gradient Markov chain Monte Carlo methods","type":"publication"},{"authors":["Niladri S. Chatterji","Behnam Neyshabur","Hanie Sedghi"],"categories":null,"content":"","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"73f2253aff3e3445f3363e35a905f413","permalink":"/publication/module_criticality/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/publication/module_criticality/","section":"publication","summary":"ICLR 2020 (Spotlight Talk); also appeared at Workshops on ML with Guarantees \u0026 on Science of Deep Learning, NeurIPS 2019","tags":null,"title":"The intriguing role of module criticality in the generalization of deep networks","type":"publication"},{"authors":["Niladri S. Chatterji*","Jelena Diakonikolas*","Michael I. Jordan","Peter L. Bartlett"],"categories":null,"content":"","date":1559174400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559174400,"objectID":"400353270ec0af024bfbaf4f9d6f8618","permalink":"/publication/smoothness_langevin/","publishdate":"2019-05-30T00:00:00Z","relpermalink":"/publication/smoothness_langevin/","section":"publication","summary":"AISTATS 2020","tags":null,"title":"Langevin Monte Carlo without smoothness","type":"publication"},{"authors":["Niladri S. Chatterji","Vidya Muthukumar","Peter L. Bartlett"],"categories":null,"content":"","date":1558656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558656000,"objectID":"69295113247999f9aa1345d774285cc2","permalink":"/publication/model_selection/","publishdate":"2019-05-24T00:00:00Z","relpermalink":"/publication/model_selection/","section":"publication","summary":"AISTATS 2020","tags":null,"title":"OSOM: A simultaneously optimal algorithm for multi-armed and linear contextual bandits","type":"publication"},{"authors":["Yi-An Ma","Niladri S. Chatterji","Xiang Cheng","Nicolas Flammarion","Peter L. Bartlett","Michael I. Jordan"],"categories":null,"content":"","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549238400,"objectID":"ff49bdec7d5428dd67134c57d9bd4ccc","permalink":"/publication/nesterov_mcmc/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/publication/nesterov_mcmc/","section":"publication","summary":"Bernoulli","tags":null,"title":"Is there an analog of Nesterov acceleration for MCMC?","type":"publication"},{"authors":["Xiang Cheng","Niladri S. Chatterji","Yasin Abbasi-Yadkori","Peter L. Bartlett","Michael I. Jordan"],"categories":null,"content":"","date":1531872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531872000,"objectID":"7cb5984b55d51c056b02a02c432538c6","permalink":"/publication/sharp_rates/","publishdate":"2018-07-18T00:00:00Z","relpermalink":"/publication/sharp_rates/","section":"publication","summary":"arXiv pre-print","tags":null,"title":"Sharp convergence rates for Langevin dynamics in the nonconvex setting","type":"publication"},{"authors":["Aldo Pacchiano*","Niladri S. Chatterji*","Peter L. Bartlett"],"categories":null,"content":"","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"89a9aae83583c6244f312a78bed8cd42","permalink":"/publication/online_kernel_learning/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/publication/online_kernel_learning/","section":"publication","summary":"ICML 2019 (Long Talk)","tags":null,"title":"Online learning with kernel losses","type":"publication"},{"authors":["Niladri S. Chatterji","Nicolas Flammarion","Yi-An Ma","Peter L. Bartlett","Michael I. Jordan"],"categories":null,"content":"","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518652800,"objectID":"0614612bc874835944a65fc7d20256ce","permalink":"/publication/langevin_variancereduction/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/publication/langevin_variancereduction/","section":"publication","summary":"ICML 2018","tags":null,"title":"On the theory of variance reduction for stochastic gradient Monte Carlo","type":"publication"},{"authors":["Xiang Cheng*","Niladri S. Chatterji*","Peter L. Bartlett","Michael I. Jordan"],"categories":null,"content":"","date":1499817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499817600,"objectID":"9c60b887da178211781b261c55e7920e","permalink":"/publication/underdamped/","publishdate":"2017-07-12T00:00:00Z","relpermalink":"/publication/underdamped/","section":"publication","summary":"COLT 2018","tags":null,"title":"Underdamped Langevin MCMC: A non-asymptotic analysis","type":"publication"},{"authors":["Niladri S. Chatterji","Peter L. Bartlett"],"categories":null,"content":"","date":1494374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494374400,"objectID":"774fa0f5a9b82fd000f2b57a75c513a0","permalink":"/publication/alternatingminimization/","publishdate":"2017-05-10T00:00:00Z","relpermalink":"/publication/alternatingminimization/","section":"publication","summary":"NIPS 2017","tags":null,"title":"Alternating minimization for dictionary learning: Local convergence guarantees","type":"publication"},{"authors":["Niladri S. Chatterji","Ashwin Tulapurkar","Bhaskaran Muralidharan"],"categories":null,"content":"","date":1417996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417996800,"objectID":"ad702f005d907325d0cec121a88b0ef4","permalink":"/publication/enhancement_spin_torque/","publishdate":"2014-12-08T00:00:00Z","relpermalink":"/publication/enhancement_spin_torque/","section":"publication","summary":"Applied Physics Letters","tags":null,"title":"Enhancement of spin-transfer torque switching via resonant tunneling","type":"publication"}]